{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fe5dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "265748a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e743ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd,4*config.n_embd)\n",
    "        self.gelu = nn.GELU(approximate='tanh')\n",
    "        self.c_proj = nn.Linear(4*config.n_embd, config.n_embd)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "            .view(1, 1, config.block_size, config.block_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2) \n",
    "\n",
    "        k = k.view(B, T, self.n_head, C//self.n_head).transpose(1, 2) \n",
    "        q = q.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C//self.n_head).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1))) \n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf')) # type: ignore\n",
    "\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        y = att @ v \n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf01680",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPT2Config:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size,config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size,config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(config.n_embd,config.vocab_size,bias=False)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,model_type):\n",
    "        \"loads pretrained gpt-2 model weights from huggingface\"\n",
    "        assert model_type in {'gpt2','gpt2-medium','gpt2-large','gpt2-xl'}\n",
    "        from transformers import GPT2LMHeadModel\n",
    "    \n",
    "        print(f\"loading weights from pretrained gpt: {model_type}\")\n",
    "    \n",
    "        config_args = {\n",
    "            'gpt2': dict(n_layer=12,n_head=12,n_embd=768),\n",
    "            'gpt2-medium': dict(n_layer=24,n_head=16,n_embd=1024),\n",
    "            'gpt2-large': dict(n_layer=36,n_head=20,n_embd=1280),\n",
    "            'gpt2-xl': dict(n_layer=48,n_head=25,n_embd=1600),\n",
    "        }[model_type]\n",
    "    \n",
    "        config_args['vocab_size'] = 50257\n",
    "        config_args['block_size'] = 1024\n",
    "    \n",
    "        config = GPT2Config(**config_args)\n",
    "        model = GPT(config=config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
    "    \n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "    \n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
    "        transposed = ['attn.c_attn.weight','attn.c_proj.weight','mlp.c_fc.weight','mlp.c_proj.weight']\n",
    "    \n",
    "        assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} vs {len(sd_keys)}\"\n",
    "    \n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "    \n",
    "            else:\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "    \n",
    "        return model\n",
    "        \n",
    "    def forward(self,x):\n",
    "        batch_size,seq_len = x.size()\n",
    "    \n",
    "        assert seq_len<=self.config.block_size, f\"can't forward\"\n",
    "        pos = torch.arange(0,seq_len,dtype=torch.long,device=x.device)\n",
    "        pos_emb = self.transformer.wpe(pos) # type: ignore\n",
    "        tok_emb = self.transformer.wte(x) # pyright: ignore[reportCallIssue]\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.transformer.h: # type: ignore\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x) # type: ignore\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6d4761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b11f41f03c14378a87f482018637556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962870097791495ebddaf93aa1ab73cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963092f5089248b19dcfb908ce9ae8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> The meaning of life is not so much that its members, those who are on it; but how they are united to one body. This is the most important question. For for the human race it is more important to keep alive an organism than to allow life to pass through it. As a result, every organism needs the presence of other organisms. In that sense, the organism in order to thrive is to die and must survive on itself and not be replaced by another organismâ€”if there is other\n",
      "> The meaning of life or other fundamental things in the universe?\"\n",
      "\n",
      "- An example of this, in a series of events from the Old Testament, which is referenced as scripture for the first time:\n",
      "\n",
      "\"[L]ew that it be a matter of life that I should have, and I should have no other means of obtaining it\"\n",
      "\n",
      "- This refers to the fact that human beings want to possess every life they can.\n",
      "\n",
      "- \"A number of the prophets spoke of that\n",
      "> The meaning of life in the Jewish religion is eternal and unchanging\" (Dukhulam 5:20).\n",
      "\n",
      "In their view, the Quran is a holy book, not an allegorical document. Therefore, as stated above, it's not a religious text (such as a Quran), and God isn't supposed to be teaching its message to us.\n",
      "\n",
      "The idea that the Quran and the Hebrew Bible are equivalent, that it's like a divine divine book that can tell us\n",
      "> The meaning of life is often found in the fact of life. However, this is not an accurate translation of the meaning of life in all of its many meanings. (J. R. Carrington 1997: 2:3) As mentioned above, the word life is used for life and the word life means life and death. (J. R. Carrington 1997: 2:3)\n",
      "\n",
      "In the context of the foregoing, living and living \"have the same meaning,\" and life does\n",
      "> The meaning of life has a meaning; it is the reality of all possible things.\n",
      "\n",
      "And it has a meaning for the good of humans. I want to show that and show him the good of humankind, and show him the good of the world. I want to show men what it means to be good. And I want to show them the world in which they can learn and to go to. I want to show them the world in which they can get their dreams in. I want\n"
     ]
    }
   ],
   "source": [
    "num_return_seq = 5\n",
    "max_length = 100\n",
    "\n",
    "\n",
    "model = GPT.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "model.to('cuda')\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(\"The meaning of life\")\n",
    "tokens = torch.tensor(tokens,dtype= torch.long)\n",
    "tokens = tokens.unsqueeze(0).repeat(num_return_seq,1)\n",
    "x = tokens.to('cuda')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "while x.size(1)<max_length:\n",
    "    with torch.no_grad():\n",
    "        logits = model(x)\n",
    "        logits = logits[:,-1,:]\n",
    "        probs = F.softmax(logits,dim = -1)\n",
    "        topk_probs, topk_indices = torch.topk(probs,50,dim=-1)\n",
    "        ix = torch.multinomial(topk_probs,1)\n",
    "        xcol = torch.gather(topk_indices,-1,ix)\n",
    "        x = torch.cat((x,xcol),dim=1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_return_seq):\n",
    "    tokens = x[i,:max_length].tolist()\n",
    "    decoded = enc.decode(tokens)\n",
    "    print(\">\",decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eff20fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50257, 768])\n",
      "torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "sd_hf = model.state_dict()\n",
    "print(sd_hf[\"lm_head.weight\"].shape)\n",
    "print(sd_hf[\"transformer.wte.weight\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0318574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
