{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "718d348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1054bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    d_model = 512 # embeded dim\n",
    "    n_h = 8 # number of head\n",
    "    seq_len = 512 # sequence length\n",
    "    batch_size = 8 # batch size\n",
    "    dff = 2048 # hidden dim\n",
    "    dp = 0.1 # dropout\n",
    "    n_layer = 6 # number of layer\n",
    "    vocab_size = 37000\n",
    "    k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4396c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, config, decoder=True):\n",
    "        super().__init__()\n",
    "        assert config.d_model % config.n_h == 0\n",
    "        self.config = config\n",
    "        self.decoder = decoder\n",
    "        self.c_attn = nn.Linear(config.d_model, config.d_model * 3)\n",
    "        self.c_proj = nn.Linear(config.d_model, config.d_model)\n",
    "        self.dropout = nn.Dropout(config.dp)\n",
    "        \n",
    "        d_k = config.d_model // config.n_h\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.tril(torch.ones(config.seq_len, config.seq_len)).view(1, 1, config.seq_len, config.seq_len)\n",
    "        )   \n",
    "        self.look_up_table_k = nn.Parameter(torch.zeros((2 * config.k + 1, d_k)))\n",
    "        self.look_up_table_v = nn.Parameter(torch.zeros((2 * config.k + 1, d_k)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(d_model, dim=2)\n",
    "        d_k = k.size(-1) // self.config.n_h\n",
    "        \n",
    "        q = q.view(batch_size, seq_len, self.config.n_h, d_k).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.config.n_h, d_k).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.config.n_h, d_k).transpose(1, 2)\n",
    "        \n",
    "        i = torch.arange(seq_len, device=x.device).unsqueeze(1)\n",
    "        j = torch.arange(seq_len, device=x.device).unsqueeze(0) \n",
    "        \n",
    "        relative_pos = torch.clamp(j - i, -self.config.k, self.config.k) + self.config.k\n",
    "        \n",
    "        aij_k = self.look_up_table_k[relative_pos]\n",
    "        aij_v = self.look_up_table_v[relative_pos]\n",
    "        \n",
    "        raw_attention_score = q @ k.transpose(-2, -1)\n",
    "        relative_pos_score = torch.einsum('bhid,ijd->bhij', q, aij_k)\n",
    "        \n",
    "        attention_score = raw_attention_score + relative_pos_score\n",
    "        attention_score = attention_score * (1.0 / math.sqrt(d_k))\n",
    "        \n",
    "        if self.decoder:\n",
    "            attention_score = attention_score.masked_fill(\n",
    "                self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf')\n",
    "            )\n",
    "\n",
    "        attention_score = F.softmax(attention_score, dim=-1)\n",
    "        attention_score = self.dropout(attention_score)\n",
    "        \n",
    "        standard_out = attention_score @ v\n",
    "        relative_v_out = torch.einsum('bhij,ijd->bhid', attention_score, aij_v)\n",
    "        \n",
    "        out = standard_out + relative_v_out\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "        out = self.c_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "716fe445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "selfattn = CausalAttention(config=ModelConfig,decoder=False)\n",
    "\n",
    "\n",
    "x = torch.randn((ModelConfig.batch_size,ModelConfig.seq_len,ModelConfig.d_model))\n",
    "print(selfattn(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176e0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b0af3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
