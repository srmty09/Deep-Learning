{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN9XG5l/LQ+/s7vdGzkYdyz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srmty09/Deep-Learning/blob/main/Fine_Tuning/unsloth_finetuning_blog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning Using Unsloth.\n",
        "\n",
        "1. Model Loading."
      ],
      "metadata": {
        "id": "62mGmln3LI4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "id": "J8D1qr8LLs0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bW6hKPOtLF-i"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "# fastlanguage model is where all the language model present.\n",
        "import torch\n",
        "max_seq_len = 512\n",
        "load_in_4bit = True # for using 4bit quantized model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of 4 bit quantized models:\n",
        "1. \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
        "2. \"unsloth/Mistral-Small-Instruct-2409\"\n",
        "3. \"unsloth/phi-4\"\n",
        "4. \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\""
      ],
      "metadata": {
        "id": "gEFMhPVBMMGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model,tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/phi-4\", # which model we want to use\n",
        "    max_seq_length = max_seq_len,\n",
        "    load_in_4bit = load_in_4bit\n",
        "    )"
      ],
      "metadata": {
        "id": "xiK4nVwjMJxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model names ending with *unsloth-bnb-4bit* indicate they are unsloth dynamic 4bit models, these models are slightly more vram than standard BitsBytes 4bit models but offer better accuracy."
      ],
      "metadata": {
        "id": "FSH9G3GaNf_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "___\n",
        "\n",
        "- we need to create a dataset usually with 2 columns Question and answer.\n",
        "- we can create synthetically generated data and structure the dataset using chatgpt.\n",
        "- we need to know the purpose of dataset, to know for what cause we need the dataset and how we will use it."
      ],
      "metadata": {
        "id": "QI1wFGJ6OX2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat Template with Unsloth\n"
      ],
      "metadata": {
        "id": "E8Nkqc7tPkVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import CHAT_TEMPLATES\n",
        "print(list(CHAT_TEMPLATES.keys()))"
      ],
      "metadata": {
        "id": "a2bsjdilNUHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")"
      ],
      "metadata": {
        "id": "aHXaUyyPP1zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_prompts_func(examples):\n",
        "  convos = examples[\"messages\"]\n",
        "  texts = [tokenizer.apply_chat_template(convo,tokenize=False,add_generation_prompt=False) for convo in convos]\n",
        "  return {\"text\":texts,}"
      ],
      "metadata": {
        "id": "bdOIEGXDQdCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = {\n",
        "    \"messages\": [\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"What is Bayes theorem?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Bayes theorem describes conditional probability.\"}\n",
        "        ],\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Explain eigenvalues.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Eigenvalues measure scaling of linear transforms.\"}\n",
        "        ]\n",
        "    ]\n",
        "}\n"
      ],
      "metadata": {
        "id": "_NSyMZmFRIm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "formatting_prompts_func(examples)"
      ],
      "metadata": {
        "id": "ddRWLQoARMfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"HuggingFaceH4/ultrachat_200k\",split = \"train_sft\")\n",
        "\n",
        "# dataset.map is used to map the dataset to the required format to train the model.\n",
        "dataset = dataset.map(formatting_prompts_func,batched = True)"
      ],
      "metadata": {
        "id": "l-A_mFcKRPpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "Hj64I26-UMFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1][\"text\"]"
      ],
      "metadata": {
        "id": "16mBAjRQVgVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# some datasets\n",
        "1. Alpaca Dataset\n",
        "2. HuggingFaceH4/ultrachat_200k\n",
        "3."
      ],
      "metadata": {
        "id": "P7PTdVzsYc6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"vicgalle/alpaca-gpt4\",split = \"train\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "0nDxSMIlVjCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "inc6Kt1nY1Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(formatting_prompts_func,batched = True)"
      ],
      "metadata": {
        "id": "emzx96kQY8Ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uZGZVrgvctnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning Llama-3!!\n"
      ],
      "metadata": {
        "id": "Tcsi2n1nctg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"unsloth[colab-new] @ git+http://github.com/unslothai/unsloth.git\"\n",
        "!pip uninstall -y xformers\n",
        "!pip install trl==0.8.6 --no-deps"
      ],
      "metadata": {
        "id": "6D71ju8ycseo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True"
      ],
      "metadata": {
        "id": "8dSAeCaycsZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        ")"
      ],
      "metadata": {
        "id": "s3yTwn1KcsXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# peft: parameter efficient fine tuning\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "                      \"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout=0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing= \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora=False,\n",
        "    loftq_config=None\n",
        ")"
      ],
      "metadata": {
        "id": "Nvke6SHzcsUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"vicgalle/alpaca-gpt4\",split = \"train\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "EjMSolrrcsR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from unsloth import to_sharegpt\n",
        "# dataset = to_sharegpt(\n",
        "#     dataset,\n",
        "#     merged_prompt = \"{instruction}[[\\nYour input is:\\n{input}]]\",\n",
        "#     output_column_name=\"output\",\n",
        "#     conversation_extension=3,\n",
        "# )"
      ],
      "metadata": {
        "id": "dNsIFOuPcsO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "D85zfpx_csJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "id": "bxckdBPncsCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)"
      ],
      "metadata": {
        "id": "m2_A4HVUcr_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset.select(range(10000)),\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = True,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1,\n",
        "        # max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        remove_unused_columns = False,\n",
        "    ),\n",
        ")\n"
      ],
      "metadata": {
        "id": "kKDS6SHxiGmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "8qS7na-xjlmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0AlOrQwFkm16"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}